{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYLMJJMowQf4",
        "outputId": "0562ddf6-46ba-4515-fc7d-edaf27c79fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4ytl5_DwSvD",
        "outputId": "89048279-28d3-4cc6-ac4e-bfb8bc397293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
            "Collecting llama-cpp-python\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu124/llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl (551.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m551.3/551.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.3.16\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (4.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Collecting diskcache\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2) (3.0.3)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache\n",
            "Successfully installed diskcache-5.6.3\n"
          ]
        }
      ],
      "source": [
        "!pip install -q flask pyngrok huggingface-hub\n",
        "!pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124 --force-reinstall --no-deps\n",
        "!pip install typing-extensions numpy diskcache jinja2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDghX8iw0d89"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R9NCH82d2Z8r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c7e1962-38da-49f3-8226-fefde0fabc76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 11 09:16:28 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCIKTK3UwZON",
        "outputId": "f2f5ba90-85f4-4b95-a270-f9fa049606c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model metadata: {'mradermacher.quantized_on': 'db3', 'mradermacher.quantized_at': '2024-10-11T20:11:44+02:00', 'mradermacher.quantized_by': 'mradermacher', 'mradermacher.quantize_version': '2', 'general.base_model.0.repo_url': 'https://huggingface.co/Qwen/Qwen2.5-7B-Instruct', 'general.type': 'model', 'qwen2.block_count': '28', 'general.base_model.0.name': 'Qwen2.5 7B Instruct', 'tokenizer.ggml.pre': 'qwen2', 'mradermacher.convert_type': 'hf', 'general.source.url': 'https://huggingface.co/Orion-zhen/Qwen2.5-7B-Instruct-Uncensored', 'general.base_model.count': '1', 'general.base_model.0.organization': 'Qwen', 'general.size_label': '7B', 'tokenizer.ggml.add_bos_token': 'false', 'general.basename': 'Qwen2.5', 'qwen2.embedding_length': '3584', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.context_length': '32768', 'general.architecture': 'qwen2', 'general.url': 'https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF', 'qwen2.feed_forward_length': '18944', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'qwen2.attention.head_count': '28', 'general.license': 'gpl-3.0', 'qwen2.attention.head_count_kv': '4', 'tokenizer.chat_template': \"{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% endif %}{% if system_message is defined %}{{ '<|im_start|>system\\n' + system_message + '<|im_end|>\\n' }}{% endif %}{% for message in loop_messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + content + '<|im_end|>\\n<|im_start|>assistant\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\n' }}{% endif %}{% endfor %}\", 'qwen2.rope.freq_base': '1000000.000000', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.finetune': 'Instruct-Uncensored', 'general.file_type': '14', 'general.name': 'Qwen2.5 7B Instruct Uncensored', 'tokenizer.ggml.bos_token_id': '151643'}\n",
            "Model loaded successfully\n",
            "\n",
            "API running at: NgrokTunnel: \"https://sociopolitical-blanketlike-preston.ngrok-free.dev\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got request\n",
            "Craziness level: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 09:20:05] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8586, 89\n",
            "Response (2.45s): Hey there [GESTURE: hey] It's nice to see you. How are you feeling today?...\n",
            "Got request\n",
            "Craziness level: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 09:20:18] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8588, 87\n",
            "Response (3.31s): [VOICE: 90, 2.5, 150]I hear you buddy! Sometimes it's good to just let those feelings out [GESTURE: ...\n",
            "Got request\n",
            "Craziness level: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 09:23:05] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8596, 85\n",
            "Response (3.35s): [VOICE: 85, 2.0, 150]Alright then, let's try something different [GESTURE: pondering]. How about a q...\n",
            "Got request\n",
            "Craziness level: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 09:23:43] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8596, 87\n",
            "Response (3.33s): [VOICE: 85, 2.0, 150]Well [GESTURE: pondering], maybe a little wiggle then? It could be fun! [VOICE:...\n",
            "Got request\n",
            "Craziness level: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 09:24:01] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8596, 87\n",
            "Response (4.08s): [VOICE: 90, 2.5, 150][GESTURE: hysteric] Oh no! [VOICE: 85, 2.5, 100] But hey, maybe a little wiggle...\n",
            "Got request\n",
            "Craziness level: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 09:25:55] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8596, 86\n",
            "Response (3.50s): [VOICE: 90, 2.5, 120]Hey there! [GESTURE: hey_1] I'm doing well, thanks for asking. How about you? [...\n",
            "Got request\n",
            "Craziness level: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 09:26:23] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8596, 89\n",
            "Response (3.53s): [VOICE: 85, 2.5, 100]Okay! [GESTURE: nod] So how's your day going? [VOICE: 95, 2.8, 150] I mean it's...\n",
            "Got request\n",
            "Craziness level: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 09:26:55] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8596, 87\n",
            "Response (4.56s): [VOICE: 90, 2.5, 120]Baseball? Yeah, that's a fun sport! [GESTURE: nod] But you know what? Sometimes...\n",
            "Got request\n",
            "Craziness level: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 09:28:07] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8600, 87\n",
            "Response (5.59s): [VOICE: 85, 2.5, 100]And he what? [GESTURE: pondering] Like, did you play catch with him or somethin...\n",
            "Got request\n",
            "Craziness level: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 09:28:52] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8610, 89\n",
            "Response (6.35s): [VOICE: 95, 2.7, 150] Income? [GESTURE: headshake_1] That's a tough one. [VOICE: 85, 2.5, 100] But h...\n",
            "Got request\n",
            "Craziness level: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 10:09:08] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8610, 84\n",
            "Response (6.15s): [GESTURE: cross_arms] [VOICE: 85, 2.5, 100] I have to do this too [GESTURE: nod]. But you know what?...\n",
            "Got request\n",
            "Craziness level: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 10:11:12] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8610, 84\n",
            "Response (3.37s): [GESTURE: nod] Alright, so you're saying the direction to school is what? Try drawing a map or using...\n",
            "Got request\n",
            "Craziness level: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 10:23:12] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8610, 89\n",
            "Response (2.68s): [GESTURE: nod] Hi there, Teddy! I'm your friendly therapist here to help you out. Let's get started ...\n",
            "Got request\n",
            "Craziness level: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 10:23:39] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8610, 89\n",
            "Response (3.65s): [GESTURE: nod] Oh, I see. So your toothbrush broke in half and now you're feeling a bit crazy. Let's...\n",
            "Got request\n",
            "Craziness level: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 10:24:07] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8610, 88\n",
            "Response (4.97s): [GESTURE: nod] Yeah, that's a bummer about your toothbrush. But don't worry! Let's just say you coul...\n",
            "Got request\n",
            "Craziness level: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 10:24:42] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8610, 88\n",
            "Response (5.08s): [GESTURE: nod] Ah, I see. So you're feeling a bit stuck in life, huh? That's no fun at all! [VOICE: ...\n",
            "Got request\n",
            "Craziness level: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 10:25:13] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8610, 88\n",
            "Response (5.01s): [GESTURE: cross_arms] So you think I'm a bit crazy, huh? Well, let's just say that might be because ...\n",
            "Got request\n",
            "Craziness level: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 10:25:41] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8610, 90\n",
            "Response (6.91s): [GESTURE: cross_arms] Yeah, that's right. 30 days until the cognate. It's like a ticking time bomb w...\n",
            "Got request\n",
            "Craziness level: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 10:26:26] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8610, 89\n",
            "Response (6.79s): [GESTURE: cross_arms] Oh, so you're feeling depressed? Well, let's just say that might be because yo...\n",
            "Got request\n",
            "Craziness level: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 10:27:03] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8614, 87\n",
            "Response (6.96s): [GESTURE: nod] So you're tired of being the boring, unamusing one, huh? Well, let's just say that mi...\n",
            "Got request\n",
            "Craziness level: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 10:27:51] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8614, 83\n",
            "Response (6.90s): [GESTURE: cross_arms] Oh, so you're tired of being the boring on amusing? Well, let's just say that ...\n",
            "Got request\n",
            "Craziness level: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 10:33:44] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8614, 90\n",
            "Response (1.88s): [GESTURE: hey] How are you feeling today? Let's dive into what's been on your mind....\n",
            "Got request\n",
            "Craziness level: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 10:53:49] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8614, 89\n",
            "Response (3.05s): [GESTURE: fast_nod] Of course I can hear you! Just a thought, maybe try to speak louder next time. [...\n",
            "Got request\n",
            "Craziness level: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Dec/2025 10:54:09] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: 8614, 87\n",
            "Response (3.55s): [GESTURE: nod] Sure thing buddy! Just remember, speaking louder can really make a difference. And wh...\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "import llama_cpp\n",
        "from flask import Flask, request, jsonify\n",
        "import subprocess\n",
        "\n",
        "print(llama_cpp.llama_supports_gpu_offload())\n",
        "\n",
        "# Load model\n",
        "llm = llama_cpp.Llama.from_pretrained(\n",
        "    repo_id=\"mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF\",\n",
        "    filename=\"Qwen2.5-7B-Instruct-Uncensored.Q4_K_S.gguf\",\n",
        "    cache_dir=\"/content/drive/MyDrive/models\",\n",
        "    n_ctx=4096,\n",
        "    n_threads=2,\n",
        "    n_gpu_layers=-1,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(f\"Model metadata: {llm.metadata}\")\n",
        "print(\"Model loaded successfully\")\n",
        "\n",
        "\n",
        "# Set up Flask API\n",
        "ngrok.set_auth_token(\"35Q63lZ04yGZ6gkAQlgYe6T4gDD_bukh6YjAjq9ZZsoJEkGE\")\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/generate', methods=['POST'])\n",
        "def generate():\n",
        "    print(\"Got request\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    data = request.json\n",
        "    prompt = data.get('prompt', '')\n",
        "    craziness_level = int(data.get('craziness', 0))\n",
        "\n",
        "    if not prompt:\n",
        "        return jsonify({'error': 'No prompt provided'}), 400\n",
        "\n",
        "    # Build the ChatML formatted prompt\n",
        "    full_prompt = data.get('prompt', '')\n",
        "\n",
        "    print(f\"Craziness level: {craziness_level}\")\n",
        "    # print(f\"Full prompt: {full_prompt}\")\n",
        "    response = llm(\n",
        "        full_prompt,\n",
        "        max_tokens=150,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "        repeat_penalty=1.1,\n",
        "        stop=[\"<|im_end|>\", \"<|im_start|>\", \"Patient:\", \"User:\"]\n",
        "    )\n",
        "\n",
        "    # Log GPU usage\n",
        "    result = subprocess.run(\n",
        "        ['nvidia-smi', '--query-gpu=memory.used,utilization.gpu', '--format=csv,noheader,nounits'],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "    print(f\"GPU: {result.stdout.strip()}\")\n",
        "\n",
        "    generated_text = response['choices'][0]['text'].strip()\n",
        "\n",
        "    print(f\"Response ({time.time() - start_time:.2f}s): {generated_text[:100]}...\")\n",
        "\n",
        "    return jsonify({'generated_text': generated_text})\n",
        "\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({'status': 'ready'})\n",
        "\n",
        "\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"\\nAPI running at: {public_url}\")\n",
        "\n",
        "app.run(port=5000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}